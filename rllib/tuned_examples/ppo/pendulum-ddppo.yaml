pendulum-ddppo:
    env: Pendulum-v1
    run: DDPPO
    stop:
        episode_reward_mean: -300
        timesteps_total: 1000000
    config:
        framework: torch
        #rollout_fragment_length: 50
        #num_sgd_iter: 4
        #num_workers: 8
        num_gpus_per_worker: 0

        #num_envs_per_worker: 5

        observation_filter: MeanStdFilter
        #lr: 0.0001
        lambda: 0.1
        #clip_param: 0.1
        #vf_clip_param: 10.0
        entropy_coeff:
            grid_search: [0.015, 0.01, 0.02, 0.0]
        #model:
        #    vf_share_layers: true
        #vf_loss_coeff: 0.01
