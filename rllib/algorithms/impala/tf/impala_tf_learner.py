from dataclasses import dataclass
from typing import List, Mapping, Optional, Union

from ray.rllib import SampleBatch
from ray.rllib.core.rl_trainer.rl_trainer import RLTrainerHPs
from ray.rllib.core.rl_trainer.tf.tf_rl_trainer import TfRLTrainer
from ray.rllib.utils.framework import try_import_tf
from ray.rllib.utils.annotations import override
from ray.rllib.utils.typing import TensorType

from ray.rllib.core.rl_trainer.rl_trainer import RLTrainerHPs

_, tf, _ = try_import_tf()


def make_time_major(
    tensor: Union[tf.Tensor, List[tf.Tensor]],
    trajectory_len: int = None,
    recurrent_seq_len: int = None,
    drop_last: bool = False,
):
    """Swaps batch and trajectory axis.

    Args:
        tensor: A tensor or list of tensors to swap the axis of.
            NOTE: The tensor(s) must flattened along the batch axis.
        trajectory_len: The length of the trajectory being transformed.
        recurrent_seq_len: Sequence lengths if recurrent or None
        drop_last: A bool indicating whether to drop the last
        trajectory item.

    Note: Either `trajectory_len` or `recurrent_seq_len` must be set. `trajectory_len`
        should be used in cases where tensor is not produced from a
        RNN/recurrent module. `recurrent_seq_len` should be used in those cases instead.

    Returns:
        res: A tensor with swapped axes or a list of tensors with
        swapped axes.
    """
    if isinstance(tensor, list):
        return [
            make_time_major(_tensor, trajectory_len, recurrent_seq_len, drop_last)
            for _tensor in tensor
        ]

    assert bool(trajectory_len) != bool(
        recurrent_seq_len
    ), "Either trajectory_len or recurrent_seq_len must be set."

    if recurrent_seq_len:
        B = tf.shape(recurrent_seq_len)[0]
        T = tf.shape(tensor)[0] // B
    else:
        T = trajectory_len
        B = tf.shape(tensor)[0] // T
    rs = tf.reshape(tensor, tf.concat([[B, T], tf.shape(tensor)[1:]], axis=0))

    # swap B and T axes
    res = tf.transpose(rs, [1, 0] + list(range(2, 1 + int(tf.shape(tensor).shape[0]))))

    if drop_last:
        return res[:-1]
    return res


def get_log_rhos(target_action_log_probs, behaviour_action_log_probs):
    """With the selected log_probs for multi-discrete actions of behaviour
    and target policies we compute the log_rhos for calculating the vtrace."""
    t = tf.stack(target_action_log_probs)
    b = tf.stack(behaviour_action_log_probs)
    log_rhos = t - b
    return log_rhos


def vtrace_tf2(
    target_action_log_probs,
    behaviour_action_log_probs,
    discounts,
    rewards,
    values,
    bootstrap_value,
    clip_rho_threshold=1.0,
    clip_pg_rho_threshold=1.0,
):
    r"""V-trace for softmax policies.

    Calculates V-trace actor critic targets for softmax polices as described in

    "IMPALA: Scalable Distributed Deep-RL with
    Importance Weighted Actor-Learner Architectures"
    by Espeholt, Soyer, Munos et al.

    Target policy refers to the policy we are interested in improving and
    behaviour policy refers to the policy that generated the given
    rewards and actions.

    In the notation used throughout documentation and comments, T refers to the
    time dimension ranging from 0 to T-1. B refers to the batch size and
    ACTION_SPACE refers to the list of numbers each representing a number of
    actions.

    Args:
        target_action_log_probs: Action log probs from the target policy. A float32
            tensor of shape [T, B].
        behaviour_action_log_probs: Action log probs from the target behavior policy. A
            float32 tensor of shape [T, B].
        discounts: A float32 tensor of shape [T, B] with the discount encountered when 
            following the behaviour policy. This will be 0 for terminal timesteps 
            (done=True) and gamma (the discount factor) otherwise.
        rewards: A float32 tensor of shape [T, B] with the rewards generated by
            following the behaviour policy.
        values: A float32 tensor of shape [T, B] with the value function estimates
            wrt. the target policy.
        bootstrap_value: A float32 of shape [B] with the value function estimate at
            time T.
        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for
            importance weights (rho) when calculating the baseline targets (vs).
            rho^bar in the paper.
        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold
            on rho_s in \rho_s \delta log \pi(a|x) (r + \gamma v_{s+1} - V(x_s)).
    """
    # log_rhos = get_log_rhos(target_action_log_probs, behaviour_action_log_probs)
    log_rhos = target_action_log_probs - behaviour_action_log_probs
    discounts = tf.convert_to_tensor(discounts, dtype=tf.float32)
    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)
    values = tf.convert_to_tensor(values, dtype=tf.float32)
    bootstrap_value = tf.convert_to_tensor(bootstrap_value, dtype=tf.float32)
    if clip_rho_threshold is not None:
        clip_rho_threshold = tf.convert_to_tensor(clip_rho_threshold, dtype=tf.float32)
    if clip_pg_rho_threshold is not None:
        clip_pg_rho_threshold = tf.convert_to_tensor(
            clip_pg_rho_threshold, dtype=tf.float32
        )

    # Make sure tensor ranks are consistent.
    rho_rank = log_rhos.shape.ndims  # Usually 2.
    values.shape.assert_has_rank(rho_rank)
    bootstrap_value.shape.assert_has_rank(rho_rank - 1)
    discounts.shape.assert_has_rank(rho_rank)
    rewards.shape.assert_has_rank(rho_rank)
    if clip_rho_threshold is not None:
        clip_rho_threshold.shape.assert_has_rank(0)
    if clip_pg_rho_threshold is not None:
        clip_pg_rho_threshold.shape.assert_has_rank(0)

    rhos = tf.math.exp(log_rhos)
    if clip_rho_threshold is not None:
        clipped_rhos = tf.minimum(clip_rho_threshold, rhos, name="clipped_rhos")
    else:
        clipped_rhos = rhos

    cs = tf.minimum(1.0, rhos, name="cs")
    # Append bootstrapped value to get [v1, ..., v_t+1]
    values_t_plus_1 = tf.concat(
        [values[1:], tf.expand_dims(bootstrap_value, 0)], axis=0
    )

    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)

    # All sequences are reversed, computation starts from the back.
    sequences = (
        tf.reverse(discounts, axis=[0]),
        tf.reverse(cs, axis=[0]),
        tf.reverse(deltas, axis=[0]),
    )

    # V-trace vs are calculated through a scan from the back to the
    # beginning of the given trajectory.
    def scanfunc(acc, sequence_item):
        discount_t, c_t, delta_t = sequence_item
        return delta_t + discount_t * c_t * acc

    initial_values = tf.zeros_like(bootstrap_value)
    vs_minus_v_xs = tf.nest.map_structure(
        tf.stop_gradient,
        tf.scan(
            fn=scanfunc,
            elems=sequences,
            initializer=initial_values,
            parallel_iterations=1,
            name="scan",
        ),
    )
    # Reverse the results back to original order.
    vs_minus_v_xs = tf.reverse(vs_minus_v_xs, [0])

    # Add V(x_s) to get v_s.
    vs = tf.add(vs_minus_v_xs, values)

    # Advantage for policy gradient.
    vs_t_plus_1 = tf.concat([vs[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)
    if clip_pg_rho_threshold is not None:
        clipped_pg_rhos = tf.minimum(clip_pg_rho_threshold, rhos)
    else:
        clipped_pg_rhos = rhos
    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)

    # Make sure no gradients backpropagated through the returned values.
    return tf.stop_gradient(vs), tf.stop_gradient(pg_advantages)


@dataclass
class ImpalaHPs(RLTrainerHPs):
    rollout_frag_or_episode_len: int = None
    recurrent_seq_len: int = None
    discount_factor: float = 0.99
    vtrace_clip_rho_threshold: float = 1.0
    vtrace_clip_pg_rho_threshold: float = 1.0
    vtrace_drop_last_ts: bool = True
    vf_loss_coeff: float = 0.5
    entropy_coeff: float = 0.01
    lr_schedule = None
    entropy_coeff_schedule = None


class ImpalaTfLearner(TfRLTrainer):
    """Implements IMPALA loss / update logic on top of TfRLTrainer.

    This class implements the IMPALA loss under `_compute_loss_per_module()`.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.entropy_coeff_scheduler = None
        if self.config.entropy_coeff_schedule:
            raise ValueError("entropy_coeff_schedule is not supported in RLTrainer yet")

        self.lr_scheduler = None
        if self.config.lr_schedule:
            raise ValueError("lr_schedule is not supported in RLTrainer yet")

        self.vtrace_clip_rho_threshold = self.config.vtrace_clip_rho_threshold
        self.vtrace_clip_pg_rho_threshold = self.config.vtrace_clip_pg_rho_threshold
        self.vtrace_drop_last_ts = self.config.vtrace_drop_last_ts
        self.vf_loss_coeff = self.config.vf_loss_coeff
        self.entropy_coeff = self.config.entropy_coeff
        self.rollout_frag_or_episode_len = self.config.rollout_frag_or_episode_len
        self.recurrent_seq_len = self.config.recurrent_seq_len
        self.discount_factor = self.config.discount_factor
        assert (
            self.rollout_frag_or_episode_len is not None
            or self.recurrent_seq_len is not None
        ) and not (self.rollout_frag_or_episode_len and self.recurrent_seq_len), (
            "Either rollout_frag_or_episode_len or recurrent_seq_len"
            " must be set in the IMPALA HParams. "
        )

    @override(TfRLTrainer)
    def _compute_loss_per_module(
        self, module_id: str, batch: SampleBatch, fwd_out: Mapping[str, TensorType]
    ) -> TensorType:
        values = fwd_out[SampleBatch.VF_PREDS]
        behavior_policy_dist = batch[SampleBatch.ACTION_DIST]
        target_policy_dist = fwd_out[SampleBatch.ACTION_DIST]

        # the actions are discrete actions
        if len(batch[SampleBatch.ACTIONS].shape) == 1:
            batched_actions = tf.expand_dims(batch[SampleBatch.ACTIONS], axis=1)
        else:
            batched_actions = batch[SampleBatch.ACTIONS]
        # this is probably a horribly inefficient way to do this. I should be able to
        # compute this in a batch fashion
        behaviour_actions_logp = tf.stack(
            [
                tf.squeeze(dist.logp(action))
                for dist, action in zip(behavior_policy_dist, batched_actions)
            ]
        )
        target_actions_logp = target_policy_dist.logp(batch[SampleBatch.ACTIONS])

        behaviour_actions_logp_time_major = make_time_major(
            behaviour_actions_logp,
            self.rollout_frag_or_episode_len,
            self.recurrent_seq_len,
            self.vtrace_drop_last_ts,
        )
        target_actions_logp_time_major = make_time_major(
            target_actions_logp,
            self.rollout_frag_or_episode_len,
            self.recurrent_seq_len,
            self.vtrace_drop_last_ts,
        )
        values_time_major = make_time_major(
            values,
            self.rollout_frag_or_episode_len,
            self.recurrent_seq_len,
            self.vtrace_drop_last_ts,
        )
        bootstrap_value = values_time_major[-1]
        rewards_time_major = make_time_major(
            batch[SampleBatch.REWARDS],
            self.rollout_frag_or_episode_len,
            self.recurrent_seq_len,
            self.vtrace_drop_last_ts,
        )

        # how to compute discouts?
        # should they be pre computed?
        discounts_time_major = (1 - make_time_major(
            batch[SampleBatch.TERMINATEDS],
            self.rollout_frag_or_episode_len,
            self.recurrent_seq_len,
            self.vtrace_drop_last_ts,
        )) * self.discount_factor
        vtrace_adjusted_target_values, pg_advantages = vtrace_tf2(
            target_action_log_probs=target_actions_logp_time_major,
            behaviour_action_log_probs=behaviour_actions_logp_time_major,
            rewards=rewards_time_major,
            values=values_time_major,
            bootstrap_value=bootstrap_value,
            clip_pg_rho_threshold=self.vtrace_clip_pg_rho_threshold,
            clip_rho_threshold=self.vtrace_clip_rho_threshold,
            discounts=discounts_time_major
        )

        # The policy gradients loss.
        pi_loss = -tf.reduce_sum(target_actions_logp_time_major * pg_advantages)

        # The baseline loss.
        delta = values_time_major - vtrace_adjusted_target_values
        vf_loss = 0.5 * tf.reduce_sum(tf.math.pow(delta, 2.0))

        # The entropy loss.
        entropy_loss = - tf.reduce_sum(target_actions_logp_time_major)

        # The summed weighted loss.
        total_loss = (
            pi_loss + vf_loss * self.vf_loss_coeff + entropy_loss * self.entropy_coeff
        )
        return {self.TOTAL_LOSS_KEY: total_loss, "pi_loss": pi_loss, "vf_loss": vf_loss}
