{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fe8246",
   "metadata": {},
   "source": [
    "# Offline reinforcement learning with Ray AIR\n",
    "In this example, we'll train a reinforcement learning agent using offline training.\n",
    "\n",
    "Offline training means that the data from the environment (and the actions performed by the agent) have been stored on disk. In contrast, online training samples experiences live by interacting with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8d8ac",
   "metadata": {},
   "source": [
    "Let's start with installing our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \"ray[rllib]\" gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b1b55",
   "metadata": {},
   "source": [
    "Now we can run some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.air import Checkpoint\n",
    "from ray.air.config import RunConfig\n",
    "from ray.train.rl.rl_predictor import RLPredictor\n",
    "from ray.train.rl.rl_trainer import RLTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.air.result import Result\n",
    "from ray.rllib.algorithms.bc import BC\n",
    "from ray.tune.tuner import Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fe936",
   "metadata": {},
   "source": [
    "We will be training on offline data - this means we have full agent trajectories stored somewhere on disk and want to train on these past experiences.\n",
    "\n",
    "Usually this data could come from external systems, or a database of historical data. But for this example, we'll generate some offline data ourselves and store it using RLlibs `output_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeed761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offline_data(path: str):\n",
    "    print(f\"Generating offline data for training at {path}\")\n",
    "    trainer = RLTrainer(\n",
    "        algorithm=\"PPO\",\n",
    "        run_config=RunConfig(stop={\"timesteps_total\": 5000}),\n",
    "        config={\n",
    "            \"env\": \"CartPole-v1\",\n",
    "            \"output\": \"dataset\",\n",
    "            \"output_config\": {\n",
    "                \"format\": \"json\",\n",
    "                \"path\": path,\n",
    "                \"max_num_samples_per_file\": 1,\n",
    "            },\n",
    "            \"batch_mode\": \"complete_episodes\",\n",
    "            \"framework\": \"tf\"\n",
    "        },\n",
    "    )\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca906c",
   "metadata": {},
   "source": [
    "Here we define the training function. It will create an `RLTrainer` using the `PPO` algorithm and kick off training on the `CartPole-v1` environment. It will use the offline data provided in `path` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5071ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_bc_offline(path: str, num_workers: int, use_gpu: bool = False) -> Result:\n",
    "    print(\"Starting offline training\")\n",
    "    dataset = ray.data.read_json(\n",
    "        path, parallelism=num_workers, ray_remote_args={\"num_cpus\": 1}\n",
    "    )\n",
    "\n",
    "    trainer = RLTrainer(\n",
    "        run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "        datasets={\"train\": dataset},\n",
    "        algorithm=BC,\n",
    "        config={\n",
    "            \"env\": \"CartPole-v1\",\n",
    "            \"framework\": \"tf\",\n",
    "            \"evaluation_num_workers\": 1,\n",
    "            \"evaluation_interval\": 1,\n",
    "            \"evaluation_config\": {\"input\": \"sampler\"},\n",
    "            \"framework\": \"tf\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Todo (krfricke/xwjiang): Enable checkpoint config in RunConfig\n",
    "    # result = trainer.fit()\n",
    "    tuner = Tuner(\n",
    "        trainer,\n",
    "        _tuner_kwargs={\"checkpoint_at_end\": True},\n",
    "    )\n",
    "    result = tuner.fit()[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935cdee",
   "metadata": {},
   "source": [
    "Once we trained our RL policy, we want to evaluate it on a fresh environment. For this, we will also define a utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_using_checkpoint(checkpoint: Checkpoint, num_episodes) -> list:\n",
    "    predictor = RLPredictor.from_checkpoint(checkpoint)\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        reward = 0.0\n",
    "        terminated = truncated = False\n",
    "        while not terminated and not truncated:\n",
    "            action = predictor.predict(np.array([obs]))\n",
    "            obs, r, terminated, truncated, _ = env.step(action[0])\n",
    "            reward += r\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4bebe",
   "metadata": {},
   "source": [
    "Let's put it all together. First, we initialize Ray and create the offline data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=8)\n",
    "\n",
    "path = \"/tmp/out\"\n",
    "generate_offline_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7534d5c",
   "metadata": {},
   "source": [
    "Then, we run training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_rl_bc_offline(path=path, num_workers=2, use_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7f318",
   "metadata": {},
   "source": [
    "And then, using the obtained checkpoint, we evaluate the policy on a fresh environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e412cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 3\n",
    "\n",
    "rewards = evaluate_using_checkpoint(result.checkpoint, num_episodes=num_eval_episodes)\n",
    "print(f\"Average reward over {num_eval_episodes} episodes: \" f\"{np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b264f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
